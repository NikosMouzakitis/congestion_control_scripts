Then, repeat with Vegas, the delay-based congestion control algorithm. On romeo, run

sudo modprobe tcp_vegas  
sudo sysctl -w net.ipv4.tcp_allowed_congestion_control="reno cubic vegas"  
then, run

on juliet: iperf3 -s -1
on romeo terminal 1: bash ss-output.sh 10.10.2.100
on romeo terminal 2: iperf3 -c juliet -t 60 -C vegas
on romeo terminal 3: ping juliet -c 50
When it finishes, use Ctrl+C to stop the ss-output script. Make a note of the iperf3 throughput and the round trip time estimated by ping during the TCP Vegas flow.

One problem with TCP Vegas is that it does not work well when it shares a bottleneck link with a TCP Reno flow (or other loss-based flow). To see how this works, we will send two TCP flows through the bottleneck router: one TCP Reno flow, and one TCP Vegas flow.

We will need two iperf3 servers running on juliet, on two different ports. In one terminal on juliet, run

iperf3 -s -1  
to start an iperf3 server on the default port 5201, and in a second terminal on juliet, run

iperf3 -s -1 -p 5301  
to start an iperf3 server on port 5301.

You'll need two terminal windows on romeo. In one of them, run

iperf3 -c juliet -t 60 -C vegas  
and a few seconds afterwards, in the second, run

iperf3 -c juliet -t 60 -C reno -p 5301  
Make a note of the throughput reported by iperf3 for each flow.

Additional exercises: TCP BBR
A more recent congestion control proposed by Google, called TCP BBR, tries to maximize throughput and at the same time minimize queuing delay in the network. You can read more about it in the TCP BBR paper.

To use the BBR congestion control for your experiment, on romeo, run

sudo modprobe tcp_bbr  
sudo sysctl -w net.ipv4.tcp_allowed_congestion_control="reno cubic vegas bbr"  
This will load the Linux kernel module for TCP BBR.

Then, repeat the other steps in the Generating Data section above, but with the iperf3 command

iperf3 -c juliet -P 3 -t 60 -C bbr
on "romeo".

BBR doesn't use a slow start threshold, so you won't be able to use the same data visualization script (which assumes that there will be an ssthresh field in the data), but you can create a similar plot on your own. The results will look something like this:



Note that BBR overall maintains a lower CWND than Cubic or Reno, because it wants to minimize queue occupancy. But you'll see in the iperf3 output that it still achieves a similar throughput (about 1Mbps total shared between the 3 flows). Also, if you look at the raw ss data for the BBR and the Reno/Cubic flows, you'll note that the BBR flows see a much lower RTT, since they do not fill the queue.

Additional exercises: Explicit congestion notification (ECN)
Finally, we'll try an experiment with explicit congestion notification. Explicit congestion notification (ECN) is a feature that allows routers to explicitly signal to a TCP sender when there is congestion. This allows the sender to reduce its congestion window before the router is forced to drop packets, reducing retransmissions. It can also help the router maintain a minimal queue, which reduces queuing delay.

ECN involves both layer 2 and layer 3, and it requires support from both transport layer endpoints (sender and receiver) and routers along the path traversed by the packets.

We will use ECN together with active queue management, which monitors the queuing delay. At the router, configure a queue in both directions that will mark packets when the queuing delay exceeds 10ms:

iface_0=$(ip route get 10.10.1.100 | grep -oP "(?<=dev )[^ ]+")
sudo tc qdisc del dev $iface_0 root  
sudo tc qdisc add dev $iface_0 root handle 1: htb default 3  
sudo tc class add dev $iface_0 parent 1: classid 1:3 htb rate 1Mbit  
sudo tc qdisc add dev $iface_0 parent 1:3 handle 3:  codel limit 100 target 10ms ecn

iface_1=$(ip route get 10.10.2.100 | grep -oP "(?<=dev )[^ ]+")
sudo tc qdisc del dev $iface_1 root  
sudo tc qdisc add dev $iface_1 root handle 1: htb default 3  
sudo tc class add dev $iface_1 parent 1: classid 1:3 htb rate 1Mbit  
sudo tc qdisc add dev $iface_1 parent 1:3 handle 3:  codel limit 100 target 10ms ecn
On romeo and juliet, enable ECN in TCP by running

sudo sysctl -w net.ipv4.tcp_ecn=1  
Next, we'll prepare to capture the TCP flow. On both end hosts, romeo and juliet, run:

sudo tcpdump -s 80 -i $(ip route get 10.10.1.1 | grep -oP "(?<=dev )[^ ]+") 'tcp' -w $(hostname -s)-tcp-ecn.pcap
ECN uses two flags in the TCP header: the ECN Echo (ECE) flag, and the Congestion Window Reduced (CWR) flag. It also uses two ECN bits in the DiffServ field of the IP header. Here is how these header fields are used:

During the connection establishment phase of TCP, both endpoints indicate to the other that they support ECN. First, one host sends an ECN-setup SYN packet: it sets the ECE and CWR flags in the TCP header of the SYN. Then, the other host response with an ECN-setup SYN-ACK packet: it sets the ECE flag (but not the CWR flag) in the TCP header of the SYN-ACK.
In any subsequent packets that carry data (not pure ACKs!), the sender will set the two ECN bits in the IP header to either 10 or 01. Either of these flag values will indicate to the routers along the path that this data packet uses an ECN-capable transport.
If the router wants to signal to the TCP sender that there is congestion - for example, if the queue at the router is starting to fill up - then it sets the two ECN bits in the IP header to 11 before forwarding the packet to the destination. This is a "Congestion Experienced" signal.
If the receiver gets a data packet with the CE signal (the ECN bits in the IP header are set to 11), the receiver will set the ECN-Echo (ECE) flag in the TCP header of the ACK for that packet.
When the sender gets the ACK with the ECE flag set, it will reduce its CWND. Then it will set the Congestion Window Reduced (CWR) flag in the TCP header of the next packet.
With the tcpdump running, we can now run the experiment. In a second terminal on juliet, run

iperf3 -s -1  
In a second terminal on romeo, run

iperf3 -c juliet -t 60 -C reno  
and finally, in a third terminal on romeo, run

ping -c 50 juliet  
When the experiment finishes, compare the delay performance of Reno with ECN (this experiment) to your previous experiment showing the delay performance without ECN.

Also, transfer the packet captures to your laptop with scp, and look for the ECN-related fields in the IP header and TCP header, during connection establishment and during data transfer.


